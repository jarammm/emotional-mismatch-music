{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b0a951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from skimage.transform import resize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from pytorch_msssim import ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c3428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resized_ssm(audio_path, target_size=128, threshold=0.8):\n",
    "    y, sr = librosa.load(audio_path)\n",
    "\n",
    "    chroma = librosa.feature.chroma_cqt(y=y, sr=sr) # (12, T)\n",
    "    ssm = cosine_similarity(chroma.T) # (T, T)\n",
    "\n",
    "    resized_ssm = resize(ssm, (target_size, target_size),\n",
    "                          mode='reflect', anti_aliasing=True, preserve_range=True)\n",
    "\n",
    "    return resized_ssm\n",
    "\n",
    "ssm_imgs = []\n",
    "labels = []\n",
    "filenames = []\n",
    "\n",
    "# emotional match + mismatch\n",
    "for label in [\"0\", \"1\", \"2\", \"3\"]:\n",
    "    dir_path = os.path.join(\"temp_wavs\", label)\n",
    "    for fname in tqdm(os.listdir(dir_path)):\n",
    "        if not fname.endswith(\".wav\"):\n",
    "            continue\n",
    "        path = os.path.join(dir_path, fname)\n",
    "        ssm = get_resized_ssm(path) # (128, 128)\n",
    "        ssm_imgs.append(ssm[None, :, :]) # (1, 128, 128)\n",
    "        labels.append(label)\n",
    "        filenames.append(fname)\n",
    "\n",
    "# chorus\n",
    "chorus_dir = \"chorus\"\n",
    "for fname in tqdm(os.listdir(chorus_dir)):\n",
    "    if not fname.endswith(\".mp3\"):\n",
    "        continue\n",
    "    path = os.path.join(chorus_dir, fname)\n",
    "    ssm = get_resized_ssm(path)\n",
    "    ssm_imgs.append(ssm[None, :, :])\n",
    "    labels.append(\"chorus\")\n",
    "    filenames.append(fname)\n",
    "\n",
    "ssm_imgs = np.array(ssm_imgs) # (B, 1, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8587ce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez(\"ssm_data.npz\", imgs=ssm_imgs, labels=labels, filenames=filenames)\n",
    "\n",
    "loaded = np.load(\"ssm_data.npz\", allow_pickle=True)\n",
    "ssm_imgs = loaded[\"imgs\"]\n",
    "labels = loaded[\"labels\"]\n",
    "filenames = loaded[\"filenames\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9e2f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSM_AutoEncoder_Skip(nn.Module):\n",
    "    def __init__(self, bottleneck_dim=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.bottleneck = nn.Linear(32*16*16, bottleneck_dim)\n",
    "        self.unbottleneck = nn.Linear(bottleneck_dim, 32*16*16)\n",
    "\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16, 8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x): # ssm_img (B, 1, 128, 128)\n",
    "        e1 = self.enc1(x) # (B, 8, 64, 64)\n",
    "        e2 = self.enc2(e1) # (B, 16, 32, 32)\n",
    "        e3 = self.enc3(e2) # (B, 32, 16, 16)\n",
    "\n",
    "        z = self.dropout(self.bottleneck(e3.view(x.size(0), -1)))\n",
    "        x_hat =self.unbottleneck(z).view(-1, 32, 16, 16)\n",
    "        x_hat = x_hat + e3 \n",
    "       \n",
    "        d1 = self.dec1(x_hat + e3) # (B, 16, 32, 32)\n",
    "        d2 = self.dec2(d1 + e2) # (B, 8, 64, 64)\n",
    "        d3 = self.dec3(d2 + e1) # (B, 1, 128, 128)\n",
    "\n",
    "        return d3, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0f63b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_reconstruction(model, ssm_tensor, n_samples=5):\n",
    "    \"\"\"\n",
    "    model: 오토인코더\n",
    "    ssm_tensor: (B, 1, 128, 128) torch.Tensor, normalized (0~1)\n",
    "    n_samples: 몇 개의 이미지 비교할지\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        decoded, _ = model(ssm_tensor)  # (B, 1, 128, 128)\n",
    "\n",
    "    decoded = decoded.cpu().numpy()\n",
    "    originals = ssm_tensor.cpu().numpy()\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        \n",
    "        # 원본\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(originals[i][0], cmap='magma', origin='lower', aspect='auto')\n",
    "        plt.title(\"Original SSM\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        # 복원본\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(decoded[i][0], cmap='magma', origin='lower', aspect='auto')\n",
    "        plt.title(\"Reconstructed SSM\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef46b7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SSM_AutoEncoder_Skip(64)\n",
    "model.load_state_dict(torch.load(\"ssm_autoencoder_sc_150.pt\", map_location=\"cpu\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff8ec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_reconstruction(model, ssm_tensor, n_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05767d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixed_loss(recon, target):\n",
    "    \"\"\"\n",
    "    recon, target: (B, 1, 128, 128)\n",
    "    \"\"\"\n",
    "    mse = F.mse_loss(recon, target)\n",
    "    ssim_loss = 1 - ssim(recon, target, data_range=1.0, size_average=True)\n",
    "    return 0.7 * mse + 0.3 * ssim_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6934ae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssm_tensor = torch.tensor(ssm_imgs, dtype=torch.float32)\n",
    "\n",
    "model = SSM_AutoEncoder_Skip(bottleneck_dim=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(150):\n",
    "    model.train()\n",
    "    output, z = model(ssm_tensor) # (128, 128) SSM, bottleneck layer\n",
    "    loss = mixed_loss(output, ssm_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    _, z = model(ssm_tensor) # (B, 64)\n",
    "    z_np = z.detach().numpy()\n",
    "\n",
    "# torch.save(model.state_dict(), \"ssm_autoencoder_all.pt\")\n",
    "# np.save(\"z_np_all.npy\", z_np)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "tsne_result = tsne.fit_transform(z_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93dd6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter valid labels (0,1,2,3 only)\n",
    "valid_labels = ['0', '1', '2', '3']\n",
    "valid_indices = [i for i, label in enumerate(labels) if label in valid_labels]\n",
    "\n",
    "labels_filtered = np.array([int(labels[i]) for i in valid_indices])\n",
    "z_filtered = z[valid_indices]\n",
    "\n",
    "# t-SNE\n",
    "tsne = TSNE(n_components=2)\n",
    "z_2d = tsne.fit_transform(z_filtered)\n",
    "\n",
    "label_map = {\n",
    "    0: \"PN\",\n",
    "    1: \"NP\",\n",
    "    2: \"PP\", \n",
    "    3: \"NN\",\n",
    "}\n",
    "\n",
    "color_map = {\n",
    "    0: \"red\",\n",
    "    1: \"orange\",\n",
    "    2: \"blue\",\n",
    "    3: \"green\",\n",
    "}\n",
    "\n",
    "def normalize_z2d(z_2d):\n",
    "    z_min = z_2d.min(axis=0)\n",
    "    z_max = z_2d.max(axis=0)\n",
    "    return (z_2d - z_min) / (z_max - z_min)\n",
    "\n",
    "z_2d_norm = normalize_z2d(z_2d)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in sorted(label_map.keys()):\n",
    "    idx = labels_filtered == i\n",
    "    plt.scatter(\n",
    "        z_2d_norm[idx, 0], z_2d_norm[idx, 1],\n",
    "        c=color_map[i],\n",
    "        label=label_map[i],\n",
    "        alpha=0.7\n",
    "    )\n",
    "plt.title(\"t-SNE of SSM Bottleneck Embeddings\")\n",
    "plt.legend(title=\"Emotion Pair\")\n",
    "# plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8ed5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=6, random_state=42)\n",
    "clusters = kmeans.fit_predict(z_2d_norm)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'x': z_2d_norm[:, 0],\n",
    "    'y': z_2d_norm[:, 1],\n",
    "    'label': labels_filtered,\n",
    "    'cluster': clusters\n",
    "})\n",
    "\n",
    "label_map = {0: 'PN', 1: 'NP', 2: 'PP', 3: 'NN'}\n",
    "df['emotion'] = df['label'].map(label_map)\n",
    "\n",
    "cluster_dist = pd.crosstab(df['cluster'], df['emotion'], normalize='index')\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.heatmap(cluster_dist, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Cluster-wise Emotion Pair Distribution\")\n",
    "plt.xlabel(\"Emotion Pair\")\n",
    "plt.ylabel(\"Cluster ID\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
